main.py
Namespace(accumulate_step=8, config='', epochs=100, gen_max_len=300, max_lr=0.0002, mle_weight=0.9, model_pt='model_weight_path', model_type='google/pegasus-x-base', no_repeat_ngram_size=2, num_beams=8, report_freq=1, rl_weight=0.1, seed=970903, test_batch_size=2, source_max_len=4096, train_batch_size=1, warmup_steps=1000)

PegasusXForConditionalGeneration(
  (model): PegasusXModel(
    (shared): Embedding(96103, 768)
    (encoder): PegasusXEncoder(
      (embed_tokens): Embedding(96103, 768)
      (embed_global): Embedding(128, 768)
      (embed_positions): PegasusXSinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-11): 12 x PegasusXEncoderLayer(
          (self_attn): PegasusXGlobalLocalAttention(
            (k_proj): Linear(in_features=768, out_features=768, bias=False)
            (v_proj): Linear(in_features=768, out_features=768, bias=False)
            (q_proj): Linear(in_features=768, out_features=768, bias=False)
            (out_proj): Linear(in_features=768, out_features=768, bias=False)
          )
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (global_self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (activation_fn): ReLU()
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): PegasusXDecoder(
      (embed_tokens): Embedding(96103, 768)
      (embed_positions): PegasusXSinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-11): 12 x PegasusXDecoderLayer(
          (self_attn): PegasusXAttention(
            (k_proj): Linear(in_features=768, out_features=768, bias=False)
            (v_proj): Linear(in_features=768, out_features=768, bias=False)
            (q_proj): Linear(in_features=768, out_features=768, bias=False)
            (out_proj): Linear(in_features=768, out_features=768, bias=False)
          )
          (activation_fn): ReLU()
          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): PegasusXAttention(
            (k_proj): Linear(in_features=768, out_features=768, bias=False)
            (v_proj): Linear(in_features=768, out_features=768, bias=False)
            (q_proj): Linear(in_features=768, out_features=768, bias=False)
            (out_proj): Linear(in_features=768, out_features=768, bias=False)
          )
          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
  )
  (lm_head): Linear(in_features=768, out_features=96103, bias=False)
)

